{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bite47df22450924690a2d4ffeb3905dd4e",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregando as bibliotecas necessárias\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando dataset\n",
    "data = keras.datasets.imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Splittando entre textos de teste e textos de treino\n",
    "#O num words é para ele selecionar as palavras que ocorrem mais de 10000 vezes no dataset\n",
    "(train_data, train_labels), (test_data, test_labels) = data.load_data(num_words=10000)\n",
    "\n",
    "#Você vai obter um vetor enorme de números\n",
    "#Cada número se refere a uma palavra, como um dicionário\n",
    "f\"{train_data[0]}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Essa função eu não entendi direito, mas ele, resumidamente tá pegando o vetor cheio de número (integer encoded) e traduzindo para texto\n",
    "#Aqui ele tá pegando o dicionário pra \"traduzir\" os números do vetor\n",
    "_word_index = keras.datasets.imdb.get_word_index()\n",
    "\n",
    "word_index = {k:(v+3) for k,v in _word_index.items()}\n",
    "#Isso é o que iremos inserir caso não houver caracter nenhum no texto\n",
    "word_index[\"<PAD>\"] = 0\n",
    "#Esse parâmetro fala aonde devemos começar a ler no texto\n",
    "word_index[\"<START>\"] = 1\n",
    "#Caso tenha caracteres desconhecidos, ele irá printar UNK\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "#Essa vai ser a função chamada para decodificar o integer do predict\n",
    "def decode_review(text):\n",
    "\treturn \" \".join([reverse_word_index.get(i, \"?\") for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nesse dataset, temos texto de diferentes tamanhos, o que é inadimissível para a neural network, já que ela \"precisa\" ter um tamanho fixo, logo, vamos cortar ou preencher os textos que não possuem esse tamanho\n",
    "#Caso o texto seja menor: será preenchido. Caso o texto seja maior: será cortado\n",
    "#1º parâmetro - Qual o data que queremos alterar\n",
    "#value - O que queremos colocar, caso o texto seja menor que 250\n",
    "#3º parâmetro - Decidir cortar o que vem depois do 250º caracter, ou antes\n",
    "#maxlen - número de caracteres que desejamos na saída\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value=word_index[\"<PAD>\"], padding=\"post\", maxlen=250)\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data, value=word_index[\"<PAD>\"], padding=\"post\", maxlen=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, None, 16)          160000    \n_________________________________________________________________\nglobal_average_pooling1d_1 ( (None, 16)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 16)                272       \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 160,289\nTrainable params: 160,289\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "#Criando o modelo\n",
    "model = keras.Sequential()\n",
    "#Tendo 2 palavras com mesmo sentido como great day e good day, para nós, parece a mesma coisa, mas a rede neural irá pegar dois números diferentes para great e good, logo, o Embedding virá para determinar o significado de CADA palavra na sentença mapeando cada palavra em um espaço de vetor, ou seja, irá agrupar palavras com sentido similar\n",
    "#Olhando as palavras que circundam as frases, ele irá criar vetores com diferentes angulos (em relação a x) para cada palavra, caso a rede neural perceba que ambas palavras possuem mesmo vizinhos (ou, consequentemente, mesmo significado), ele irá tentar apeoximar esses dois vetores\n",
    "#O 10000 é o número de palavras (no caso, números) que irão entrar na Layer, e 16 é as 16 dimensões de saída que será utilizadas para representar os vetores criados\n",
    "model.add(keras.layers.Embedding(10000,16))\n",
    "#Como após o Embedding ele sai com muitas dimensões, o Globar Average realiza um scale down de dimensões para facilitar para a rede neural analisar tudo\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "#Essa layer são 16 neurônios todos conectados com o Global Average\n",
    "model.add(keras.layers.Dense(16,activation=\"relu\"))\n",
    "#Esse é o Output, onde sairá um valor entre 0 e 1 que falará se o movie review é positivo ou negativo\n",
    "model.add(keras.layers.Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "#Isto mostra o design da sua rede\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#O binary cross entropy é uma cost function visionada para apenas 2 valores na saída,sendo representados por 0 ou 1\n",
    "model.compile(optimizer = \"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For this specific model we will introduce a new idea of validation data. In the last tutorial when we trained the models accuracy after each epoch on the current training data, data the model had seen before. This can be problematic as it is highly possible the a model can simply memorize input data and its related output and the accuracy will affect how the model is modified as it trains. So to avoid this issue we will sperate our training data into two sections, training and validation. The model will use the validation data to check accuracy after learning from the training data. This will hopefully result in us avoiding a false confidence for our model.\n",
    "x_val = train_data[:10000]\n",
    "x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 15000 samples, validate on 10000 samples\nEpoch 1/40\n15000/15000 [==============================] - 1s 87us/sample - loss: 0.6919 - accuracy: 0.5187 - val_loss: 0.6901 - val_accuracy: 0.5651\nEpoch 2/40\n15000/15000 [==============================] - 1s 57us/sample - loss: 0.6864 - accuracy: 0.6073 - val_loss: 0.6830 - val_accuracy: 0.6154\nEpoch 3/40\n15000/15000 [==============================] - 1s 64us/sample - loss: 0.6751 - accuracy: 0.6629 - val_loss: 0.6687 - val_accuracy: 0.7280\nEpoch 4/40\n15000/15000 [==============================] - 1s 81us/sample - loss: 0.6553 - accuracy: 0.7301 - val_loss: 0.6463 - val_accuracy: 0.7553\nEpoch 5/40\n15000/15000 [==============================] - 1s 95us/sample - loss: 0.6260 - accuracy: 0.7770 - val_loss: 0.6160 - val_accuracy: 0.7654\nEpoch 6/40\n15000/15000 [==============================] - 1s 83us/sample - loss: 0.5880 - accuracy: 0.7993 - val_loss: 0.5777 - val_accuracy: 0.7983\nEpoch 7/40\n15000/15000 [==============================] - 1s 78us/sample - loss: 0.5442 - accuracy: 0.8215 - val_loss: 0.5364 - val_accuracy: 0.8150\nEpoch 8/40\n15000/15000 [==============================] - 1s 76us/sample - loss: 0.4983 - accuracy: 0.8417 - val_loss: 0.4957 - val_accuracy: 0.8283\nEpoch 9/40\n15000/15000 [==============================] - 1s 80us/sample - loss: 0.4545 - accuracy: 0.8539 - val_loss: 0.4581 - val_accuracy: 0.8389\nEpoch 10/40\n15000/15000 [==============================] - 1s 91us/sample - loss: 0.4147 - accuracy: 0.8695 - val_loss: 0.4263 - val_accuracy: 0.8476\nEpoch 11/40\n15000/15000 [==============================] - 1s 83us/sample - loss: 0.3804 - accuracy: 0.8766 - val_loss: 0.3989 - val_accuracy: 0.8557\nEpoch 12/40\n15000/15000 [==============================] - 1s 73us/sample - loss: 0.3511 - accuracy: 0.8863 - val_loss: 0.3771 - val_accuracy: 0.8603\nEpoch 13/40\n15000/15000 [==============================] - 1s 71us/sample - loss: 0.3263 - accuracy: 0.8934 - val_loss: 0.3598 - val_accuracy: 0.8645\nEpoch 14/40\n15000/15000 [==============================] - 1s 67us/sample - loss: 0.3051 - accuracy: 0.8979 - val_loss: 0.3457 - val_accuracy: 0.8695\nEpoch 15/40\n15000/15000 [==============================] - 1s 75us/sample - loss: 0.2869 - accuracy: 0.9023 - val_loss: 0.3339 - val_accuracy: 0.8725\nEpoch 16/40\n15000/15000 [==============================] - 1s 76us/sample - loss: 0.2708 - accuracy: 0.9076 - val_loss: 0.3241 - val_accuracy: 0.8755\nEpoch 17/40\n15000/15000 [==============================] - 1s 89us/sample - loss: 0.2565 - accuracy: 0.9139 - val_loss: 0.3161 - val_accuracy: 0.8791\nEpoch 18/40\n15000/15000 [==============================] - 1s 92us/sample - loss: 0.2433 - accuracy: 0.9179 - val_loss: 0.3096 - val_accuracy: 0.8814\nEpoch 19/40\n15000/15000 [==============================] - 1s 85us/sample - loss: 0.2317 - accuracy: 0.9208 - val_loss: 0.3040 - val_accuracy: 0.8807\nEpoch 20/40\n15000/15000 [==============================] - 1s 85us/sample - loss: 0.2209 - accuracy: 0.9242 - val_loss: 0.2995 - val_accuracy: 0.8826\nEpoch 21/40\n15000/15000 [==============================] - 1s 74us/sample - loss: 0.2108 - accuracy: 0.9281 - val_loss: 0.2957 - val_accuracy: 0.8824\nEpoch 22/40\n15000/15000 [==============================] - 1s 75us/sample - loss: 0.2015 - accuracy: 0.9315 - val_loss: 0.2927 - val_accuracy: 0.8846\nEpoch 23/40\n15000/15000 [==============================] - 1s 72us/sample - loss: 0.1925 - accuracy: 0.9352 - val_loss: 0.2906 - val_accuracy: 0.8845\nEpoch 24/40\n15000/15000 [==============================] - 1s 97us/sample - loss: 0.1847 - accuracy: 0.9392 - val_loss: 0.2887 - val_accuracy: 0.8847\nEpoch 25/40\n15000/15000 [==============================] - 1s 90us/sample - loss: 0.1770 - accuracy: 0.9438 - val_loss: 0.2873 - val_accuracy: 0.8850\nEpoch 26/40\n15000/15000 [==============================] - 1s 82us/sample - loss: 0.1706 - accuracy: 0.9446 - val_loss: 0.2871 - val_accuracy: 0.8853\nEpoch 27/40\n15000/15000 [==============================] - 1s 68us/sample - loss: 0.1630 - accuracy: 0.9487 - val_loss: 0.2867 - val_accuracy: 0.8841\nEpoch 28/40\n15000/15000 [==============================] - 1s 80us/sample - loss: 0.1566 - accuracy: 0.9513 - val_loss: 0.2864 - val_accuracy: 0.8840\nEpoch 29/40\n15000/15000 [==============================] - 1s 82us/sample - loss: 0.1507 - accuracy: 0.9531 - val_loss: 0.2868 - val_accuracy: 0.8843\nEpoch 30/40\n15000/15000 [==============================] - 1s 95us/sample - loss: 0.1447 - accuracy: 0.9550 - val_loss: 0.2872 - val_accuracy: 0.8858\nEpoch 31/40\n15000/15000 [==============================] - 1s 75us/sample - loss: 0.1391 - accuracy: 0.9575 - val_loss: 0.2884 - val_accuracy: 0.8851\nEpoch 32/40\n15000/15000 [==============================] - 1s 80us/sample - loss: 0.1343 - accuracy: 0.9598 - val_loss: 0.2888 - val_accuracy: 0.8856\nEpoch 33/40\n15000/15000 [==============================] - 1s 78us/sample - loss: 0.1287 - accuracy: 0.9637 - val_loss: 0.2903 - val_accuracy: 0.8861\nEpoch 34/40\n15000/15000 [==============================] - 1s 75us/sample - loss: 0.1242 - accuracy: 0.9642 - val_loss: 0.2916 - val_accuracy: 0.8861\nEpoch 35/40\n15000/15000 [==============================] - 1s 71us/sample - loss: 0.1195 - accuracy: 0.9661 - val_loss: 0.2938 - val_accuracy: 0.8853\nEpoch 36/40\n15000/15000 [==============================] - 1s 69us/sample - loss: 0.1150 - accuracy: 0.9671 - val_loss: 0.2962 - val_accuracy: 0.8853\nEpoch 37/40\n15000/15000 [==============================] - 1s 75us/sample - loss: 0.1108 - accuracy: 0.9693 - val_loss: 0.2987 - val_accuracy: 0.8837\nEpoch 38/40\n15000/15000 [==============================] - 1s 65us/sample - loss: 0.1067 - accuracy: 0.9707 - val_loss: 0.3006 - val_accuracy: 0.8836\nEpoch 39/40\n15000/15000 [==============================] - 1s 69us/sample - loss: 0.1028 - accuracy: 0.9719 - val_loss: 0.3032 - val_accuracy: 0.8848\nEpoch 40/40\n15000/15000 [==============================] - 1s 76us/sample - loss: 0.0990 - accuracy: 0.9738 - val_loss: 0.3061 - val_accuracy: 0.8832\n25000/25000 [==============================] - 1s 48us/sample - loss: 0.3268 - accuracy: 0.8712\n[0.3268436122608185, 0.87116]\n"
    }
   ],
   "source": [
    "#Batch_size --> será quantos reviews ele irá pegar simultâneamente\n",
    "fitModel = model.fit(x_train,y_train, epochs=40, batch_size=512, validation_data=(x_val,y_val),verbose=1)\n",
    "\n",
    "results = model.evaluate(test_data,test_labels)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvando o modelo criado:\n",
    "model.save(\"model.h5\")  # name it whatever you want but end with .h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregando o modelo:\n",
    "model = keras.models.load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Actual: 0'"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fazendo alguns predicts\n",
    "test_review = test_data[0]\n",
    "predict = model.predict([test_review])\n",
    "f\"Review: {decode_review(test_review)}\"\n",
    "f\"Prediction: {str(predict[0])}\"\n",
    "f\"Actual: {str(test_labels[0])}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}